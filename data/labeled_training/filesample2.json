[
  {
    "text": "Facial Affect Recognition based on Multi Architecture Encoder and Feature Fusion for the ABAW7 Challenge",
    "bbox": {
      "x0": 37.65299987792969,
      "y0": 32.7801399230957,
      "x1": 376.2376708984375,
      "y1": 82.9923095703125
    },
    "page_number": 1,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 14.35,
    "font_name": "SFBX1440",
    "is_bold": true,
    "is_italic": false,
    "word_count": 15,
    "line_count": 3,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": -42.0743408203125,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "arXiv:2407.12258v2 [cs.CV] 26 Jul 2024",
    "bbox": {
      "x0": 10.940000534057617,
      "y0": 40.91796875,
      "x1": 37.619998931884766,
      "y1": 383.11798095703125
    },
    "page_number": 1,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 20.0,
    "font_name": "Times-Roman",
    "is_bold": false,
    "is_italic": false,
    "word_count": 5,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": -277.70729064941406,
    "vertical_space_before": -42.0743408203125,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "Kang Shen 1 , Xuxiong Liu 1 , Jun Yao, Boyan Wang, Yu Wang, Yujie Guan, Xin Liu, Gengchen Li, and Xiao Sun \u0000",
    "bbox": {
      "x0": 34.55400848388672,
      "y0": 105.41069030761719,
      "x1": 379.31768798828125,
      "y1": 128.6197509765625
    },
    "page_number": 1,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 24,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 11.69573974609375,
    "vertical_space_before": -277.70729064941406,
    "label": "NONE",
    "language": "id"
  },
  {
    "text": "Hefei University of Technology Hefei, China {shenkang,liuxuxiong, yaojun, wangboyan,2023170653}@mail.hfut.edu.cn {320522038,2312789664,}@mail.hfut.edu.cn,ligengchen6599@163.com sunx@hfut.edu.cn",
    "bbox": {
      "x0": 46.91801452636719,
      "y0": 140.31549072265625,
      "x1": 367.0008850097656,
      "y1": 193.53651428222656
    },
    "page_number": 1,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFTT0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 11,
    "line_count": 5,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 25.663742065429688,
    "vertical_space_before": 11.69573974609375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "Abstract. In this paper, we present our approach to addressing the challenges of the 7th ABAW competition. The competition comprises three sub-challenges: Valence Arousal (VA) estimation, Expression (Expr) classification, and Action Unit (AU) detection. To tackle these challenges, we employ state-of-the-art models to extract powerful visual features. Subsequently, a Transformer Encoder is utilized to integrate these fea- tures for the VA, Expr, and AU sub-challenges. To mitigate the impact of varying feature dimensions, we introduce an affine module to align the features to a common dimension. Overall, our results significantly outperform the baselines.",
    "bbox": {
      "x0": 62.362022399902344,
      "y0": 219.20025634765625,
      "x1": 353.77899169921875,
      "y1": 326.79974365234375
    },
    "page_number": 1,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": true,
    "is_italic": false,
    "word_count": 93,
    "line_count": 10,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 19.91448974609375,
    "vertical_space_before": 25.663742065429688,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "1 Introduction",
    "bbox": {
      "x0": 34.01602554321289,
      "y0": 346.7142333984375,
      "x1": 128.16323852539062,
      "y1": 358.66943359375
    },
    "page_number": 1,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 11.96,
    "font_name": "SFBX1200",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 11.420562744140625,
    "vertical_space_before": 19.91448974609375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "Sentiment analysis, as a key area in pattern recognition, is driving human- computer interaction to a deeper emotional dimension. Sentiment behavior anal- ysis, through in-depth study of non-verbal signals such as human facial expres- sions, speech and body language, aims to improve the emotion understanding ability of AI systems and achieve more natural and empathetic human-computer communication. With the deepening research on sentiment behavior analysis in natural scenarios, we are gradually developing more advanced AI technologies, and these play a key role in a variety of fields such as technical human-computer interaction, healthcare, driving safety, advertising optimization, education and learning, and virtual reality. Facial Expression Recognition (FER) shows great potential in the aforemen- tioned areas, and despite significant advances in facial recognition technology, the fine-grained nuances of emotion understanding are still an unsolved chal- lenge. Facial information is the most intuitive and real element of emotional expression, therefore, we focus on facial emotion recognition and comprehen- sively present a solution aimed at addressing three major challenges of affective behavior analysis in a wild environment (ABAW): first, valence-arousal (VA) es- timation, i.e., accurately determining positive and negative emotions and their",
    "bbox": {
      "x0": 34.01602554321289,
      "y0": 370.0899963378906,
      "x1": 379.8577575683594,
      "y1": 583.2906494140625
    },
    "page_number": 1,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 189,
    "line_count": 18,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 100,
    "vertical_space_before": 11.420562744140625,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "2 Authors Suppressed Due to Excessive Length",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 11.078518867492676,
      "x1": 250.2586212158203,
      "y1": 20.044918060302734
    },
    "page_number": 2,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 7,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 16.159286499023438,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "activation levels; and second, expression (Expr) recognition, which is aimed at identifying, e.g., six basic (Anger, Disgust, Fear, Happiness, Sadness, Surprise) and Neutrality; and third, Action Unit (AU) Detection, which analyzes facial muscle movements to capture subtle facial gestures and then decode complex emotional expressions. This comprehensive solution is dedicated to improving the accuracy and utility of sentiment analysis. Based on the rich multimodal nature of the Aff-Wild 2 dataset, we are com- mitted to mining the sentiment information embedded in it and enhancing the real-world utility of the analysis method. To achieve this goal, we adopt three key strategies. First, a self-supervised Masked Auto Encoder (MAE) model is utilized to learn high-quality sentiment feature representations from large facial image datasets to optimize subsequent task performance. Second, through the Transformer-based model architecture, we effectively fuse information from mul- timodal data to facilitate the interaction of different modalities such as audio, video, and text. Finally, using an integrated learning strategy, we subdivide the dataset into multiple sub-datasets, assign them to different classifiers, and inte- grate the outputs of these classifiers in order to obtain more comprehensive and accurate sentiment analysis results. Our proposed method is innovative and breakthrough in three aspects: (1) We integrate and optimize a large-scale facial expression dataset, and through fine-tuned processing, we successfully construct an efficient facial ex- pression feature extractor, which significantly improves the performance of our model on specific facial expression recognition tasks. (2) We introduced a Transformer-based multimodal integration model to ad- dress the three sub-challenges of the ABAW competition. This model effectively facilitates complementarity and fusion between different modal data, thus en- hancing the extraction and analysis of expression features. (3) We adopted an integrated learning strategy to address the need for sen- timent analysis in different scenarios. This approach trains classifiers indepen- dently on multiple sub-datasets with unique contexts, and improves the overall accuracy and generalization ability by integrating the prediction results of these classifiers, which ensures the stable operation of our model in diverse application scenarios.",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 36.20420455932617,
      "x1": 379.857666015625,
      "y1": 428.73260498046875
    },
    "page_number": 2,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 335,
    "line_count": 33,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 18.74560546875,
    "vertical_space_before": 16.159286499023438,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "2 Related Work",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 447.47821044921875,
      "x1": 136.00582885742188,
      "y1": 459.43341064453125
    },
    "page_number": 2,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 11.96,
    "font_name": "SFBX1200",
    "is_bold": true,
    "is_italic": false,
    "word_count": 3,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 11.8961181640625,
    "vertical_space_before": 18.74560546875,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "2.1 Multimodal Features",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 471.32952880859375,
      "x1": 165.05409240722656,
      "y1": 481.2921447753906
    },
    "page_number": 2,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 3,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 8.349853515625,
    "vertical_space_before": 11.8961181640625,
    "label": "NONE",
    "language": "ro"
  },
  {
    "text": "In prior editions of the ABAW [ 9 – 21 , 36 ], a plethora of multimodal features, en- compassing visual, auditory, and textual characteristics, have been extensively employed. By extracting and analyzing these multifaceted features, the perfor- mance of affective behavior analysis tasks can be significantly enhanced. In the visual modality, facial expressions constitute a crucial aspect for un- derstanding and analyzing emotions. The human face is represented by a specific set of facial muscle movements, known as Action Units (AUs) [ 25 ], which have been widely adopted in the study of facial expressions.",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 489.6419982910156,
      "x1": 379.8675842285156,
      "y1": 583.2905883789062
    },
    "page_number": 2,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 96,
    "line_count": 8,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 100,
    "vertical_space_before": 8.349853515625,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "Abbreviated paper title 3",
    "bbox": {
      "x0": 251.7899932861328,
      "y0": 11.078518867492676,
      "x1": 379.85711669921875,
      "y1": 20.044918060302734
    },
    "page_number": 3,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 4,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 16.159286499023438,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "Within the context of affective computing, auditory features—typically en- compassing energy characteristics, temporal domain features, frequency domain features, psychoacoustic features, and perceptual features—have been exten- sively utilized. They have demonstrated promising performance in tasks such as expression classification and VA estimation [ 23 , 30 , 39 ]. These features can be extracted using pyAudioAnalysis [ 3 ], and akin to visual features, deep learning has also been extensively applied in the extraction of acoustic characteristics. In the previous iterations of the ABAW competition [ 6 , 7 , 10 , 26 , 37 , 38 ], nu- merous teams have leveraged multimodal features. Meng et al. [ 26 ] proposed a model that utilized both auditory and visual features, ultimately securing the top position in the vocal affect estimation track. To fully exploit affective in- formation in the wild, Zhang et al. [ 38 ] harnessed multimodal information from images, audio, and text, and proposed a unified multimodal framework for Action Unit detection and expression recognition. This framework achieved the highest scores in both tasks. These methodologies have convincingly demonstrated the efficacy of multimodal features in the realm of affective behavior analysis tasks.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 36.20420455932617,
      "x1": 379.8675537109375,
      "y1": 225.49472045898438
    },
    "page_number": 3,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 194,
    "line_count": 16,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 17.78692626953125,
    "vertical_space_before": 16.159286499023438,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "2.2 Multimodal Structure",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 243.28164672851562,
      "x1": 170.46376037597656,
      "y1": 253.24424743652344
    },
    "page_number": 3,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 3,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 7.8298797607421875,
    "vertical_space_before": 17.78692626953125,
    "label": "NONE",
    "language": "ro"
  },
  {
    "text": "In early studies, [ 28 , 35 ] employed connected multimodal features to train Sup- port Vector Machine (SVM) models, which were found to be deficient in effec- tively modeling multimodal information. Recent research on multimodal affective analysis has primarily utilized deep learning models to simulate the interaction of information within and between modalities. [ 32 ] developed a neural network model known as Visual Aspect Attention Network (VistaNet), which leverages visual information as a sentence-level alignment source. This multimodal archi- tecture enables the model to focus more on these sentences when classifying emo- tions. Presently, the use of transformers for multimodal learning has become the mainstream in multimodal algorithms. In the domain of image-text matching, ALBEF [ 22 ], to some extent inspired by the CLIP [ 29 ] model, has introduced the concept of multimodal contrastive learning into multimodal models, achieving a unification of multimodal contrastive learning and multimodal fusion learning. In previous iterations of the ABAW [ 8 , 31 , 34 , 38 ], have harnessed the trans- former architecture and achieved remarkable results.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 261.0741271972656,
      "x1": 379.84765625,
      "y1": 438.4086608886719
    },
    "page_number": 3,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 178,
    "line_count": 15,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 18.224609375,
    "vertical_space_before": 7.8298797607421875,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "3 Feature Extraction",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 456.6332702636719,
      "x1": 165.2841033935547,
      "y1": 468.5884704589844
    },
    "page_number": 3,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 11.96,
    "font_name": "SFBX1200",
    "is_bold": true,
    "is_italic": false,
    "word_count": 3,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 11.3775634765625,
    "vertical_space_before": 18.224609375,
    "label": "NONE",
    "language": "ro"
  },
  {
    "text": "We fuse features from different neural networks to obtain more reliable emotional features and utilize these fused features for downstream tasks. By combining in- formation from various feature extraction models such as ResNet and POSTER, we achieve a more comprehensive and accurate representation of emotions.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 479.9660339355469,
      "x1": 379.84765625,
      "y1": 525.7936401367188
    },
    "page_number": 3,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 45,
    "line_count": 4,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 17.78790283203125,
    "vertical_space_before": 11.3775634765625,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "3.1 Resnet-18",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 543.58154296875,
      "x1": 109.81145477294922,
      "y1": 553.5441284179688
    },
    "page_number": 3,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 7.8289794921875,
    "vertical_space_before": 17.78790283203125,
    "label": "NONE",
    "language": "da"
  },
  {
    "text": "ResNet [ 5 ](He et al. 2016) is a deep convolutional neural network (CNN) ar- chitecture designed to address the common issues of vanishing gradients and",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 561.3731079101562,
      "x1": 379.8575744628906,
      "y1": 583.2906494140625
    },
    "page_number": 3,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 26,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 100,
    "vertical_space_before": 7.8289794921875,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "4 Authors Suppressed Due to Excessive Length",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 11.078518867492676,
      "x1": 250.2586212158203,
      "y1": 20.044918060302734
    },
    "page_number": 4,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 7,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 16.159286499023438,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "exploding gradients during the training of deep neural networks. Its core idea is the introduction of residual blocks, which incorporate skip connections, making the network easier to train. Instead of directly learning the mapping of each layer, the output of the residual block learns the residual between the input and output. This structure effectively mitigates the vanishing gradient problem. The pre-trained model of ResNet-18 can be used as a feature extractor, which first pretained on the MS-Celeb-1M [ 4 ], and finally obtain a 512-dimensional visual feature vector. transforming images into high-dimensional feature vectors for use in other machine learning tasks, such as image retrieval and image similarity computation.",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 36.20420455932617,
      "x1": 379.85760498046875,
      "y1": 153.7637939453125
    },
    "page_number": 4,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 109,
    "line_count": 10,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 16.790924072265625,
    "vertical_space_before": 16.159286499023438,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "3.2 POSTER",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 170.55471801757812,
      "x1": 106.99205017089844,
      "y1": 180.51731872558594
    },
    "page_number": 4,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 6.8328857421875,
    "vertical_space_before": 16.790924072265625,
    "label": "NONE",
    "language": "tl"
  },
  {
    "text": "The two-stream Pyramid crOss-fuSion TransformER network (POSTER) [ 40 ] is a novel deep learning model specifically designed for video understanding tasks, such as action recognition and video classification. POSTER combines a pyra- mid structure with a two-stream architecture, leveraging cross-layer fusion and transformer networks to enhance video understanding performance. Extensive experimental results demonstrate that POSTER outperforms SOTA methods on RAF-DB with 92.05%, AffectNet [ 27 ] (7 cls) with 67.31%, and AffectNet (8cls) with 63.34%, respectively . The dimension of the visual feature vectors is 768.",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 187.35020446777344,
      "x1": 379.8577575683594,
      "y1": 292.9547119140625
    },
    "page_number": 4,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 87,
    "line_count": 9,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 16.790924072265625,
    "vertical_space_before": 6.8328857421875,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "3.3 POSTER2",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 309.7456359863281,
      "x1": 112.72054290771484,
      "y1": 319.708251953125
    },
    "page_number": 4,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 6.832855224609375,
    "vertical_space_before": 16.790924072265625,
    "label": "NONE",
    "language": "lt"
  },
  {
    "text": "The proposed POSTER2 [ 24 ] aims to improve upon the complex architecture of POSTER, which focus primarily on improvements in cross-fusion, dual-stream design, and multi-scale feature extraction.In cross-fusion, POSTER2 replaces traditional cross-attention mechanisms with window-based cross-attention mech- anisms. The dual-stream design eliminates the branch from images to landmarks. Regarding multi-scale feature extraction, POSTER2 integrates multi-scale fea- tures of images and landmarks, replacing POSTER’s pyramid design to allevi- ate computational burden. Experimental results demonstrate that POSTER2 achieves state-of-the-art Facial Expression Recognition (FER) performance on multiple benchmark datasets with minimal computational cost. It retains the same visual feature dimensionality of 768 as POSTER.",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 326.5411071777344,
      "x1": 379.837646484375,
      "y1": 456.0556945800781
    },
    "page_number": 4,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 102,
    "line_count": 11,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 16.790924072265625,
    "vertical_space_before": 6.832855224609375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "3.4 FAU",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 472.84661865234375,
      "x1": 83.19139862060547,
      "y1": 482.8092346191406
    },
    "page_number": 4,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 6.83282470703125,
    "vertical_space_before": 16.790924072265625,
    "label": "NONE",
    "language": "de"
  },
  {
    "text": "Facial Action Units (FAU), originally introduced by Ekman and Friesen [ 2 ], are strongly associated with the expression of emotions .In the fields of computer vision and human-computer interaction, Facial Action Units (FAUs) are widely employed in the development of facial expression analysis and facial recognition systems. By detecting and recognizing combinations of individual FAUs, it is possible to infer overall facial expressions and their emotional meanings. We utilize the OpenFace [ 1 ] framework (Baltrusaitis et al., 2018) for FAU feature extraction, resulting in a 17-dimensional feature vector.",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 489.6420593261719,
      "x1": 379.8974914550781,
      "y1": 583.2907104492188
    },
    "page_number": 4,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 90,
    "line_count": 8,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 100,
    "vertical_space_before": 6.83282470703125,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "Abbreviated paper title 5",
    "bbox": {
      "x0": 251.7899932861328,
      "y0": 11.078518867492676,
      "x1": 379.85711669921875,
      "y1": 20.044918060302734
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 4,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 74.80706405639648,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "PosterV2",
    "bbox": {
      "x0": 115.86559295654297,
      "y0": 94.85198211669922,
      "x1": 142.66477966308594,
      "y1": 101.4897689819336
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 6.64,
    "font_name": "SimSun",
    "is_bold": false,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 1,
    "vertical_space_after": 2.165557861328125,
    "vertical_space_before": 74.80706405639648,
    "label": "NONE",
    "language": "pt"
  },
  {
    "text": "Valence",
    "bbox": {
      "x0": 342.9674987792969,
      "y0": 103.65532684326172,
      "x1": 360.793701171875,
      "y1": 110.38214111328125
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 4.7,
    "font_name": "Arial-BoldMT",
    "is_bold": true,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 2,
    "vertical_space_after": -5.8215179443359375,
    "vertical_space_before": 2.165557861328125,
    "label": "NONE",
    "language": "es"
  },
  {
    "text": "Transformer1",
    "bbox": {
      "x0": 275.6448059082031,
      "y0": 104.56062316894531,
      "x1": 315.8611145019531,
      "y1": 111.19841003417969
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 6.64,
    "font_name": "SimSun",
    "is_bold": false,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 2,
    "vertical_space_after": 22.25238037109375,
    "vertical_space_before": -5.8215179443359375,
    "label": "NONE",
    "language": "da"
  },
  {
    "text": "Transformer2",
    "bbox": {
      "x0": 275.38165283203125,
      "y0": 133.45079040527344,
      "x1": 315.5979919433594,
      "y1": 140.0885772705078
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 6.64,
    "font_name": "SimSun",
    "is_bold": false,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 2,
    "vertical_space_after": -5.9136199951171875,
    "vertical_space_before": 22.25238037109375,
    "label": "NONE",
    "language": "da"
  },
  {
    "text": "Arousal",
    "bbox": {
      "x0": 343.46356201171875,
      "y0": 134.17495727539062,
      "x1": 361.0176086425781,
      "y1": 140.90176391601562
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 4.7,
    "font_name": "Arial-BoldMT",
    "is_bold": true,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 2,
    "vertical_space_after": 6.59942626953125,
    "vertical_space_before": -5.9136199951171875,
    "label": "NONE",
    "language": "fr"
  },
  {
    "text": "C",
    "bbox": {
      "x0": 191.36151123046875,
      "y0": 147.50119018554688,
      "x1": 197.5013427734375,
      "y1": 159.67030334472656
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.5,
    "font_name": "Arial-BoldMT",
    "is_bold": true,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 1,
    "vertical_space_after": -9.424728393554688,
    "vertical_space_before": 6.59942626953125,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "ResNet18",
    "bbox": {
      "x0": 116.12543487548828,
      "y0": 150.24557495117188,
      "x1": 142.9246063232422,
      "y1": 156.88336181640625
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 6.64,
    "font_name": "SimSun",
    "is_bold": false,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 1,
    "vertical_space_after": 11.598861694335938,
    "vertical_space_before": -9.424728393554688,
    "label": "NONE",
    "language": "de"
  },
  {
    "text": "Transformer3",
    "bbox": {
      "x0": 275.6414794921875,
      "y0": 168.4822235107422,
      "x1": 315.8578186035156,
      "y1": 175.12001037597656
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 6.64,
    "font_name": "SimSun",
    "is_bold": false,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 2,
    "vertical_space_after": -6.10260009765625,
    "vertical_space_before": 11.598861694335938,
    "label": "NONE",
    "language": "da"
  },
  {
    "text": "Expression",
    "bbox": {
      "x0": 341.4556884765625,
      "y0": 169.0174102783203,
      "x1": 366.88214111328125,
      "y1": 175.7442169189453
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 4.7,
    "font_name": "Arial-BoldMT",
    "is_bold": true,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 2,
    "vertical_space_after": 28.092025756835938,
    "vertical_space_before": -6.10260009765625,
    "label": "NONE",
    "language": "fr"
  },
  {
    "text": "AU",
    "bbox": {
      "x0": 347.219482421875,
      "y0": 203.83624267578125,
      "x1": 354.0149841308594,
      "y1": 210.56304931640625
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 4.7,
    "font_name": "Arial-BoldMT",
    "is_bold": true,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 2,
    "vertical_space_after": -6.482940673828125,
    "vertical_space_before": 28.092025756835938,
    "label": "NONE",
    "language": "tl"
  },
  {
    "text": "FAU",
    "bbox": {
      "x0": 124.29864501953125,
      "y0": 204.08010864257812,
      "x1": 134.32618713378906,
      "y1": 210.7178955078125
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 6.64,
    "font_name": "SimSun",
    "is_bold": false,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 1,
    "vertical_space_after": -5.7869110107421875,
    "vertical_space_before": -6.482940673828125,
    "label": "NONE",
    "language": "de"
  },
  {
    "text": "Transformer4",
    "bbox": {
      "x0": 275.6414794921875,
      "y0": 204.9309844970703,
      "x1": 315.8578186035156,
      "y1": 211.5687713623047
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 6.64,
    "font_name": "SimSun",
    "is_bold": false,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": true,
    "column": 2,
    "vertical_space_after": 21.089736938476562,
    "vertical_space_before": -5.7869110107421875,
    "label": "NONE",
    "language": "da"
  },
  {
    "text": "Concatenation C",
    "bbox": {
      "x0": 95.10185241699219,
      "y0": 232.65850830078125,
      "x1": 162.5228271484375,
      "y1": 244.82762145996094
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.5,
    "font_name": "Arial-BoldMT",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": true,
    "column": 1,
    "vertical_space_after": 54.94169616699219,
    "vertical_space_before": 21.089736938476562,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "Fig. 1: The overall framework of our proposed method. Visual Extractors contain EAC, ResNet18, POSTER, etc.The design of the transformer encoder is consistent with [ 33 ].",
    "bbox": {
      "x0": 34.015995025634766,
      "y0": 299.7693176269531,
      "x1": 379.85870361328125,
      "y1": 319.69696044921875
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": true,
    "is_italic": false,
    "word_count": 27,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 24.202484130859375,
    "vertical_space_before": 54.94169616699219,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "4 Method",
    "bbox": {
      "x0": 34.015995025634766,
      "y0": 343.8994445800781,
      "x1": 100.33148956298828,
      "y1": 355.8546447753906
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 11.96,
    "font_name": "SFBX1200",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 13.089569091796875,
    "vertical_space_before": 24.202484130859375,
    "label": "NONE",
    "language": "cy"
  },
  {
    "text": "The 7th ABAW encompasses a total of two challenges, and we have participated in the first challenge. Drawing upon the design of classical transformer models, our entire process consists of four stages. As illustrated in Fig. 1 Initially, we utilize existing pre-trained models or toolkits to extract visual and auditory features from each frame of the video. Subsequently, each sequence of visual or auditory features is fed into an affine module to achieve features of the same dimension. Thirdly, these features are concatenated and then input into the transformer encoder to simulate temporal relationships. Lastly, the output of the encoder is fed into the output layer to obtain the corresponding outputs. The figure illustrates the overall framework of our proposed method.",
    "bbox": {
      "x0": 34.015995025634766,
      "y0": 368.9442138671875,
      "x1": 379.8575744628906,
      "y1": 486.50384521484375
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 121,
    "line_count": 10,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 19.4998779296875,
    "vertical_space_before": 13.089569091796875,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "4.1 Affine Module",
    "bbox": {
      "x0": 34.015995025634766,
      "y0": 506.00372314453125,
      "x1": 131.81884765625,
      "y1": 515.96630859375
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 3,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 9.54193115234375,
    "vertical_space_before": 19.4998779296875,
    "label": "NONE",
    "language": "ro"
  },
  {
    "text": "In our experiments, the input consists of one or more visual features, but their feature dimensions often differ, and the discrepancies can be quite substantial. It can be observed that the EAC features span 2048 dimensions, while FAU has only 17 dimensions. We posit that excessively large dimensional disparities may diminish the effectiveness of the useful features. To address this, we have",
    "bbox": {
      "x0": 34.015995025634766,
      "y0": 525.5082397460938,
      "x1": 379.8277587890625,
      "y1": 583.2908325195312
    },
    "page_number": 5,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 62,
    "line_count": 5,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 100,
    "vertical_space_before": 9.54193115234375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "6 Authors Suppressed Due to Excessive Length",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 11.078518867492676,
      "x1": 250.2586212158203,
      "y1": 20.044918060302734
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 7,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 16.159286499023438,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "designed an affine module. For the first three challenges, we employ a linear layer to affinely transform features of varying dimensions to a uniform dimen- sion. Furthermore, adhering to the setup of classical transformer [ 33 ] models, we add positional encoding (PE) to each feature sequence to convey its contextual temporal information. The formula is as follows:",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 36.20420455932617,
      "x1": 379.8575134277344,
      "y1": 93.98784637451172
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 58,
    "line_count": 5,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 14.258049011230469,
    "vertical_space_before": 16.159286499023438,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "\\ h a t { f } _ i = (Kf_i + c) + PE \\label {eq:affine} (1)",
    "bbox": {
      "x0": 161.4449920654297,
      "y0": 108.24589538574219,
      "x1": 379.84820556640625,
      "y1": 122.33248901367188
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMMI10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 18,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 8.689407348632812,
    "vertical_space_before": 14.258049011230469,
    "label": "NONE",
    "language": "da"
  },
  {
    "text": "where f 1 , f 2 , ..., f n denote all the features, n is the number of features.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 131.0218963623047,
      "x1": 340.9597473144531,
      "y1": 142.49081420898438
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMMI10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 20,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 22.118927001953125,
    "vertical_space_before": 8.689407348632812,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "4.2 Transformer Encoder",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 164.6097412109375,
      "x1": 166.9469757080078,
      "y1": 174.5723419189453
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 3,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 13.654876708984375,
    "vertical_space_before": 22.118927001953125,
    "label": "NONE",
    "language": "da"
  },
  {
    "text": "Vectors from different feature extraction models may contain redundant or ir- relevant information. To combine these features and construct a more suitable vector that retains more useful information for downstream classification tasks, we use a basic transformer encoder [ 33 ]. In the context of classification tasks, the transformer [ 33 ] model typically employs only the encoder part. The final vector is processed to capture the contexts and interdependencies of the data components. The output from the encoder is then typically passed through one or more dense layers to perform classification tasks such as Arousal-Valence and Action Units.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 188.2272186279297,
      "x1": 379.8575439453125,
      "y1": 293.83172607421875
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 99,
    "line_count": 9,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 14.2581787109375,
    "vertical_space_before": 13.654876708984375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "\\ h a t { f } = \\ l e f t [ f_{1} ; f_{2};\\cdots ; f_{n} \\right ] (2)",
    "bbox": {
      "x0": 165.93399047851562,
      "y0": 308.08990478515625,
      "x1": 379.84820556640625,
      "y1": 321.52252197265625
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMR10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 22,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 9.3433837890625,
    "vertical_space_before": 14.2581787109375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "where [; ; ] denotes the concatenation operation.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 330.86590576171875,
      "x1": 241.33428955078125,
      "y1": 340.8408203125
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 8,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 14.258087158203125,
    "vertical_space_before": 9.3433837890625,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "T= T E ( \\hat {f}) (3)",
    "bbox": {
      "x0": 182.26800537109375,
      "y0": 355.0989074707031,
      "x1": 379.84820556640625,
      "y1": 367.70281982421875
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMR10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 7,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 10.18438720703125,
    "vertical_space_before": 14.258087158203125,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "where T denotes the temporal feature.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 377.88720703125,
      "x1": 201.79611206054688,
      "y1": 387.8498229980469
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 6,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 13.419097900390625,
    "vertical_space_before": 10.18438720703125,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "P=M d l (T) (4)",
    "bbox": {
      "x0": 179.3679962158203,
      "y0": 401.2689208984375,
      "x1": 379.84820556640625,
      "y1": 411.24383544921875
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMMI10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 5,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 13.4180908203125,
    "vertical_space_before": 13.419097900390625,
    "label": "NONE",
    "language": "ca"
  },
  {
    "text": "where Mdl represents the invoked model, and p represents the resulting proba- bility distribution.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 424.66192626953125,
      "x1": 379.8224792480469,
      "y1": 446.5918273925781
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": true,
    "word_count": 14,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 23.6129150390625,
    "vertical_space_before": 13.4180908203125,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "4.3 Loss Function",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 470.2047424316406,
      "x1": 129.38795471191406,
      "y1": 480.1673583984375
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 3,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 13.655853271484375,
    "vertical_space_before": 23.6129150390625,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "For the distinct tasks within Challenge 1, we employ various loss functions tai- lored to the specific requirements of each task. We utilize the Mean Squared Error (MSE) and CCC loss to handle VA analysis, the Cross-Entropy loss for Expression Recognition, and a Weighted Asymmetric Loss for the Action Unit (AU) problem.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 493.8232116699219,
      "x1": 379.8376770019531,
      "y1": 551.6058349609375
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 52,
    "line_count": 5,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 3.8228759765625,
    "vertical_space_before": 13.655853271484375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "{",
    "bbox": {
      "x0": 206.9110107421875,
      "y0": 555.4287109375,
      "x1": 221.3069610595703,
      "y1": 573.2079467773438
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMEX10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 1,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": -14.40106201171875,
    "vertical_space_before": 3.8228759765625,
    "label": "NONE",
    "language": "unknown"
  },
  {
    "text": "L( p , \\ h a",
    "bbox": {
      "x0": 151.50399780273438,
      "y0": 558.806884765625,
      "x1": 202.0002899169922,
      "y1": 575.509521484375
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMR10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 6,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": -11.74078369140625,
    "vertical_space_before": -14.40106201171875,
    "label": "NONE",
    "language": "ca"
  },
  {
    "text": "p }) = \\ f r a c {1}{N}\\sum _{i=1}^N (p_i-\\hat {p}_i)^2 \\label {eq:mse_va} (5)",
    "bbox": {
      "x0": 207.65301513671875,
      "y0": 563.7687377929688,
      "x1": 379.84820556640625,
      "y1": 586.6105346679688
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMR10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 15,
    "line_count": 3,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": -14.2296142578125,
    "vertical_space_before": -11.74078369140625,
    "label": "NONE",
    "language": "ca"
  },
  {
    "text": "t",
    "bbox": {
      "x0": 194.96400451660156,
      "y0": 572.3809204101562,
      "x1": 202.97393798828125,
      "y1": 582.343505859375
    },
    "page_number": 6,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMMI10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 100,
    "vertical_space_before": -14.2296142578125,
    "label": "NONE",
    "language": "vi"
  },
  {
    "text": "Abbreviated paper title 7",
    "bbox": {
      "x0": 251.7899932861328,
      "y0": 11.078518867492676,
      "x1": 379.85711669921875,
      "y1": 20.044918060302734
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 4,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 16.146961212158203,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "which p i and ˆ p i is the label and prediction of valence or arousal, N is the number of frames in a batch.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 36.19187927246094,
      "x1": 379.82342529296875,
      "y1": 58.121822357177734
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": true,
    "word_count": 25,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 9.149921417236328,
    "vertical_space_before": 16.146961212158203,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "t",
    "bbox": {
      "x0": 196.5919952392578,
      "y0": 67.27174377441406,
      "x1": 210.98794555664062,
      "y1": 85.05093383789062
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMEX10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 1,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": -17.779190063476562,
    "vertical_space_before": 9.149921417236328,
    "label": "NONE",
    "language": "vi"
  },
  {
    "text": ")",
    "bbox": {
      "x0": 212.64300537109375,
      "y0": 67.27174377441406,
      "x1": 227.03895568847656,
      "y1": 85.05093383789062
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMEX10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 1,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": -7.791145324707031,
    "vertical_space_before": -17.779190063476562,
    "label": "NONE",
    "language": "unknown"
  },
  {
    "text": "L( p , \\ h a",
    "bbox": {
      "x0": 144.91799926757812,
      "y0": 77.2597885131836,
      "x1": 194.93389892578125,
      "y1": 87.35150146484375
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMMI10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 6,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": -9.962600708007812,
    "vertical_space_before": -7.791145324707031,
    "label": "NONE",
    "language": "ca"
  },
  {
    "text": "= - \\ su m _ { i =1}^N \\sum _{j=1}^C p_{ij} \\log {\\hat {p}_{ij}} \\label {eq:crossentropy} (6)",
    "bbox": {
      "x0": 212.94400024414062,
      "y0": 77.38890075683594,
      "x1": 379.84820556640625,
      "y1": 98.45354461669922
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMMI7",
    "is_bold": false,
    "is_italic": true,
    "word_count": 18,
    "line_count": 3,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": -6.973793029785156,
    "vertical_space_before": -9.962600708007812,
    "label": "NONE",
    "language": "ca"
  },
  {
    "text": "{ p}",
    "bbox": {
      "x0": 197.33399963378906,
      "y0": 91.47975158691406,
      "x1": 210.2440948486328,
      "y1": 98.45354461669922
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 6.97,
    "font_name": "CMR7",
    "is_bold": false,
    "is_italic": true,
    "word_count": 2,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 10.987358093261719,
    "vertical_space_before": -6.973793029785156,
    "label": "NONE",
    "language": "tl"
  },
  {
    "text": "which p ij and ˆ p ij is the label and prediction of expression, N is the number of frames in a batch and C = 8 which denotes the number of expressions.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 109.44090270996094,
      "x1": 379.8360290527344,
      "y1": 131.371826171875
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": true,
    "word_count": 33,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 11.085922241210938,
    "vertical_space_before": 10.987358093261719,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "{ p",
    "bbox": {
      "x0": 154.48199462890625,
      "y0": 142.45774841308594,
      "x1": 168.87794494628906,
      "y1": 160.2369384765625
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMEX10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": -14.402023315429688,
    "vertical_space_before": 11.085922241210938,
    "label": "NONE",
    "language": "sl"
  },
  {
    "text": "L( p , \\ h a t",
    "bbox": {
      "x0": 91.32599639892578,
      "y0": 145.8349151611328,
      "x1": 149.5712890625,
      "y1": 162.53750610351562
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMR10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 7,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": -10.091720581054688,
    "vertical_space_before": -14.402023315429688,
    "label": "NONE",
    "language": "ca"
  },
  {
    "text": "} ) = - \\ f rac { 1 } {N } \\ su m _{i =1 } ^ N w_i[p_i\\log {\\hat {p}_i}+(1-p_i)\\hat {p}_i\\log {(1-\\hat {p}_i)}] \\label {eq:auloss} (7)",
    "bbox": {
      "x0": 155.22499084472656,
      "y0": 152.44578552246094,
      "x1": 379.84820556640625,
      "y1": 173.63955688476562
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "CMR10",
    "is_bold": false,
    "is_italic": true,
    "word_count": 29,
    "line_count": 3,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 10.183334350585938,
    "vertical_space_before": -10.091720581054688,
    "label": "NONE",
    "language": "ca"
  },
  {
    "text": "which ˆ p i , p i and w i are the prediction (occurrence probability), ground truth and weight of the i th AU. By the way, w i is defined by the occurrence rate of the i th AU in the whole training set.",
    "bbox": {
      "x0": 34.0159912109375,
      "y0": 183.82289123535156,
      "x1": 379.8285827636719,
      "y1": 217.70779418945312
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": true,
    "word_count": 45,
    "line_count": 3,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 20.52960205078125,
    "vertical_space_before": 10.183334350585938,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "5 Experiments",
    "bbox": {
      "x0": 34.015987396240234,
      "y0": 238.23739624023438,
      "x1": 128.27078247070312,
      "y1": 250.19259643554688
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 11.96,
    "font_name": "SFBX1200",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 13.680084228515625,
    "vertical_space_before": 20.52960205078125,
    "label": "NONE",
    "language": "ca"
  },
  {
    "text": "5.1 Dataset",
    "bbox": {
      "x0": 34.015987396240234,
      "y0": 263.8726806640625,
      "x1": 98.7131118774414,
      "y1": 273.8352966308594
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 10.134857177734375,
    "vertical_space_before": 13.680084228515625,
    "label": "NONE",
    "language": "id"
  },
  {
    "text": "The upcoming challenge will utilize the s-Aff-Wild2 database. s-Aff-Wild2 is the static version of the Aff-Wild2 database; it comprises frames selected from Aff- Wild2. In total, approximately 221K images will be employed, which include annotations on valence arousal; six basic expressions, along with a neutral state, plus an \"other\" category (encompassing emotional states not included in other categories); and 12 action units, namely AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25, and AU26. Regarding the pre-trained visual feature extractors, two datasets are men- tioned: RAF-DB: A large-scale database containing around 30,000 facial images from numerous individuals, each image annotated approximately 40 times, followed by refinement using the EM algorithm to ensure the reliability of the annotations. AffectNet: A substantial facial expression recognition dataset comprising over one million images sourced from the internet. Approximately half of these images are manually annotated with seven discrete facial expressions, as well as the intensity levels of emotional value and arousal.",
    "bbox": {
      "x0": 34.015987396240234,
      "y0": 283.97015380859375,
      "x1": 379.8874816894531,
      "y1": 473.36273193359375
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 159,
    "line_count": 16,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 20.091888427734375,
    "vertical_space_before": 10.134857177734375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "5.2 Experimental Results",
    "bbox": {
      "x0": 34.015987396240234,
      "y0": 493.4546203613281,
      "x1": 168.25210571289062,
      "y1": 503.417236328125
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFBX1000",
    "is_bold": true,
    "is_italic": false,
    "word_count": 3,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 10.1348876953125,
    "vertical_space_before": 20.091888427734375,
    "label": "NONE",
    "language": "ca"
  },
  {
    "text": "The results, as presented in the table, indicate that the combination of POSTER2, ResNet18, and FAU features excels across various metrics, including emotional Valence-Arousal, expression recognition, and Action Unit (AU) detection, thereby underscoring their robustness. Specifically, the ResNet18 architecture has a no- table influence on the recognition of facial expressions, while the FAU features are particularly impactful for the detection of Action Units.",
    "bbox": {
      "x0": 34.015987396240234,
      "y0": 513.5521240234375,
      "x1": 382.4379577636719,
      "y1": 583.2907104492188
    },
    "page_number": 7,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 63,
    "line_count": 6,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 100,
    "vertical_space_before": 10.1348876953125,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "8 Authors Suppressed Due to Excessive Length",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 11.078518867492676,
      "x1": 250.2586212158203,
      "y1": 20.044918060302734
    },
    "page_number": 8,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 7,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 15.043609619140625,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "Features Valence Arousal FER AU score EAC 0.414 0.425 0.249 0.433 1.1015 POSTER 0.439 0.347 0.247 0.423 1.063 ResNet18 0.420 0.451 0.266 0.454 1.1555 POSTER2 0.483 0.374 0.253 0.441 1.1775 ResNet18+Fau 0.443 0.393 0.281 0.468 1.167 ResNet18+POSTER2 0.462 0.412 0.315 0.452 1.204 POSTER2+POSTER+EAC 0.453 0.398 0.243 0.446 1.1145 ResNet18+POSTER2+FAU 0.503 0.432 0.319 0.493 1.2795 Table 1: The results on the validation set of Valence-Arousal Estimation, Facial Ex- pression Recognition and Action Unit (AU) Detection with different features.",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 35.08852767944336,
      "x1": 379.85577392578125,
      "y1": 154.44097900390625
    },
    "page_number": 8,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": true,
    "is_italic": false,
    "word_count": 77,
    "line_count": 56,
    "is_in_table": true,
    "column": 2,
    "vertical_space_after": 34.686492919921875,
    "vertical_space_before": 15.043609619140625,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "6 Conclusion",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 189.12747192382812,
      "x1": 118.491455078125,
      "y1": 201.08267211914062
    },
    "page_number": 8,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 11.96,
    "font_name": "SFBX1200",
    "is_bold": true,
    "is_italic": false,
    "word_count": 2,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 13.757583618164062,
    "vertical_space_before": 34.686492919921875,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "In this paper, we introduce the methodologies presented at the 7th ABAW com- petition, encompassing three distinct sub-challenges: Valence Arousal (VA) es- timation, Expression (Expr) classification, and Action Unit (AU) detection. We leveraged a robust suite of visual feature extractors and designed an affine mod- ule to standardize the varying impacts of individual feature sets. Our compre- hensive experimental protocol demonstrates that our approach significantly sur- passes the benchmarks, ensuring remarkable performance across all sub-challenges.",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 214.8402557373047,
      "x1": 386.3033142089844,
      "y1": 296.5337829589844
    },
    "page_number": 8,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 9.96,
    "font_name": "SFRM1000",
    "is_bold": false,
    "is_italic": false,
    "word_count": 75,
    "line_count": 7,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 32.558624267578125,
    "vertical_space_before": 13.757583618164062,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "References",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 329.0924072265625,
      "x1": 96.93621826171875,
      "y1": 341.047607421875
    },
    "page_number": 8,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 11.96,
    "font_name": "SFBX1200",
    "is_bold": true,
    "is_italic": false,
    "word_count": 1,
    "line_count": 1,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 13.537933349609375,
    "vertical_space_before": 32.558624267578125,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "1. Baltrusaitis, T., Zadeh, A., Lim, Y.C., Morency, L.P.: Openface 2.0: Facial behavior analysis toolkit. In: 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018). pp. 59–66. IEEE (2018) 4 2. Ekman, P., Friesen, W.V.: Facial action coding system. Environmental Psychology & Nonverbal Behavior (1978) 4 3. Giannakopoulos, T.: pyaudioanalysis: An open-source python library for audio sig- nal analysis. PloS one 10 (12) (2015) 3 4. Guo, Y., Zhang, L., Hu, Y., He, X., Gao, J.: Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14. pp. 87–102. Springer (2016) 4 5. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770–778 (2016) 3 6. Jiang, W., Wu, Y., Qiao, F., Meng, L., Deng, Y., Liu, C.: Facial action unit recog- nition with multi-models ensembling. arXiv preprint arXiv:2203.13046 (2022) 3 7. Jin, Y., Zheng, T., Gao, C., Xu, G.: A multi-modal and multi-task learning method for action unit and expression recognition. arXiv preprint arXiv:2107.04187 (2021) 3 8. Kim, J.H., Kim, N., Won, C.S.: Facial expression recognition with swin trans- former. arXiv preprint arXiv:2203.13472 (2022) 3",
    "bbox": {
      "x0": 38.62199401855469,
      "y0": 354.5855407714844,
      "x1": 379.8906555175781,
      "y1": 583.0718994140625
    },
    "page_number": 8,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": true,
    "is_italic": false,
    "word_count": 213,
    "line_count": 21,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 100,
    "vertical_space_before": 13.537933349609375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "Abbreviated paper title 9",
    "bbox": {
      "x0": 251.7899932861328,
      "y0": 11.078518867492676,
      "x1": 379.85711669921875,
      "y1": 20.044918060302734
    },
    "page_number": 9,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 4,
    "line_count": 2,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 16.936614990234375,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "9. Kollias, D., Schulc, A., Hajiyev, E., Zafeiriou, S.: Analysing affective behavior in the first abaw 2020 competition. In: 2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG). pp. 794–800 2 10. Kollias, D.: Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges. In: Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition. pp. 2328–2336 (2022) 2 , 3 11. Kollias, D.: Abaw: learning from synthetic data & multi-task learning challenges. In: European Conference on Computer Vision. pp. 157–172. Springer (2023) 2 12. Kollias, D.: Multi-label compound expression recognition: C-expr database & net- work. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition. pp. 5589–5598 (2023) 2 13. Kollias, D., Sharmanska, V., Zafeiriou, S.: Face behavior a la carte: Expressions, affect and action units in a single network. arXiv preprint arXiv:1910.11111 (2019) 2 14. Kollias, D., Sharmanska, V., Zafeiriou, S.: Distribution matching for heteroge- neous multi-task learning: a large-scale face study. arXiv preprint arXiv:2105.03790 (2021) 2 15. Kollias, D., Tzirakis, P., Baird, A., Cowen, A., Zafeiriou, S.: Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction in- tensity estimation challenges. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5888–5897 (2023) 2 16. Kollias, D., Tzirakis, P., Cowen, A., Zafeiriou, S., Shao, C., Hu, G.: The 6th affective behavior analysis in-the-wild (abaw) competition. arXiv preprint arXiv:2402.19344 (2024) 2 17. Kollias, D., Tzirakis, P., Nicolaou, M.A., Papaioannou, A., Zhao, G., Schuller, B., Kotsia, I., Zafeiriou, S.: Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond. International Journal of Computer Vision pp. 1–23 (2019) 2 18. Kollias, D., Zafeiriou, S.: Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface. arXiv preprint arXiv:1910.04855 (2019) 2 19. Kollias, D., Zafeiriou, S.: Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework. arXiv preprint arXiv:2103.15792 (2021) 2 20. Kollias, D., Zafeiriou, S.: Analysing affective behavior in the second abaw2 compe- tition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3652–3660 (2021) 2 21. Kollias, D., Zafeiriou, S., Kotsia, I., Dhall, A., Ghosh, S., Shao, C., Hu, G.: 7th abaw competition: Multi-task learning and compound expression recognition. arXiv preprint arXiv:2407.03835 (2024) 2 22. Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems 34 , 9694–9705 (2021) 3 23. Lieskovská, E., Jakubec, M., Jarina, R., Chmulík, M.: A review on speech emotion recognition using deep learning and attention mechanism. Electronics 10 (10), 1163 (2021) 3 24. Mao, J., Xu, R., Yin, X., Chang, Y., Nie, B., Huang, A.: Poster v2: A simpler and stronger facial expression recognition network. arXiv preprint arXiv:2301.12149 (2023) 4 25. Martinez, B., Valstar, M.F., Jiang, B., Pantic, M.: Automatic analysis of facial actions: A survey. IEEE transactions on affective computing 10 (3), 325–347 (2017) 2",
    "bbox": {
      "x0": 34.01598358154297,
      "y0": 36.98153305053711,
      "x1": 379.8987121582031,
      "y1": 583.0718994140625
    },
    "page_number": 9,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": true,
    "is_italic": false,
    "word_count": 490,
    "line_count": 50,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 100,
    "vertical_space_before": 16.936614990234375,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "10 Authors Suppressed Due to Excessive Length",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 11.078518867492676,
      "x1": 250.2586212158203,
      "y1": 20.044918060302734
    },
    "page_number": 10,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": false,
    "is_italic": false,
    "word_count": 7,
    "line_count": 2,
    "is_in_table": false,
    "column": 1,
    "vertical_space_after": 16.936614990234375,
    "vertical_space_before": 100,
    "label": "NONE",
    "language": "en"
  },
  {
    "text": "26. Meng, L., Liu, Y., Liu, X., Huang, Z., Jiang, W., Zhang, T., Deng, Y., Li, R., Wu, Y., Zhao, J., et al.: Multi-modal emotion estimation for in-the-wild videos. arXiv preprint arXiv:2203.13032 (2022) 3 27. Mollahosseini, A., Hasani, B., Mahoor, M.H.: Affectnet: A database for facial ex- pression, valence, and arousal computing in the wild. IEEE Transactions on Affec- tive Computing 10 (1), 18–31 (2017) 4 28. Pérez-Rosas, V., Mihalcea, R., Morency, L.P.: Utterance-level multimodal senti- ment analysis. In: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 973–982 (2013) 3 29. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021) 3 30. Stuhlsatz, A., Meyer, C., Eyben, F., Zielke, T., Meier, G., Schuller, B.: Deep neural networks for acoustic emotion recognition: Raising the benchmarks. In: 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP). pp. 5688–5691. IEEE (2011) 3 31. Tallec, G., Yvinec, E., Dapogny, A., Bailly, K.: Multi-label transformer for action unit detection. arXiv preprint arXiv:2203.12531 (2022) 3 32. Truong, Q.T., Lauw, H.W.: Vistanet: Visual aspect attention network for mul- timodal sentiment analysis. In: Proceedings of the AAAI conference on artificial intelligence. vol. 33, pp. 305–312 (2019) 3 33. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. Neural Information Processing Systems (2017) 5 , 6 34. Wang, L., Wang, S., Qi, J.: Multi-modal multi-label facial action unit detection with transformer. arXiv preprint arXiv:2203.13301 (2022) 3 35. Zadeh, A., Zellers, R., Pincus, E., Morency, L.P.: Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages. IEEE Intelligent Systems 31 (6), 82–88 (2016) 3 36. Zafeiriou, S., Kollias, D., Nicolaou, M.A., Papaioannou, A., Zhao, G., Kotsia, I.: Aff-wild: Valence and arousal ‘in-the-wild’challenge. In: Computer Vision and Pat- tern Recognition Workshops (CVPRW), 2017 IEEE Conference on. pp. 1980–1987. IEEE (2017) 2 37. Zhang, S., An, R., Ding, Y., Guan, C.: Continuous emotion recognition using visual-audio-linguistic information: A technical report for abaw3. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2376–2381 (2022) 3 38. Zhang, W., Qiu, F., Wang, S., Zeng, H., Zhang, Z., An, R., Ma, B., Ding, Y.: Transformer-based multimodal information fusion for facial expression analysis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2428–2437 (2022) 3 39. Zhang, Y., Du, J., Wang, Z., Zhang, J., Tu, Y.: Attention based fully convolutional network for speech emotion recognition. In: 2018 Asia-Pacific Signal and Informa- tion Processing Association Annual Summit and Conference (APSIPA ASC). pp. 1771–1775. IEEE (2018) 3 40. Zheng, C., Mendieta, M., Chen, C.: Poster: A pyramid cross-fusion transformer network for facial expression recognition. arXiv preprint arXiv:2204.04083 (2022) 4",
    "bbox": {
      "x0": 34.01599884033203,
      "y0": 36.98153305053711,
      "x1": 379.8818664550781,
      "y1": 571.9755249023438
    },
    "page_number": 10,
    "page_width": 413.8580017089844,
    "page_height": 615.1179809570312,
    "font_size": 8.97,
    "font_name": "SFRM0900",
    "is_bold": true,
    "is_italic": false,
    "word_count": 484,
    "line_count": 49,
    "is_in_table": false,
    "column": 2,
    "vertical_space_after": 100,
    "vertical_space_before": 16.936614990234375,
    "label": "NONE",
    "language": "en"
  }
]